---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 03-regression-variable-selection.md in _episodes_rmd/
title: "Feature selection for regression"
teaching: 45
exercises: 15
questions:
- "Why would we want to find a subset of features
  that are associated with an outcome?"
- "How should we *not* select features?"
- "How can we iteratively find a good subset of our features
  variables to use for regression?"
- "What are some risks and downsides of iterative feature
  selection?"
objectives:
- "Understand multiple regression in a biomedical context."
- "Understand how to fit a stepwise regression model."
keypoints:
- "Sets of features can be more predictive and provide
  a better explanation than a single feature alone."
- "Stepwise regression allows us to find a set of features that
  are associated with an outcome (eg, age)."
- "Stepwise regression is not very efficient."
# - "Stepwise regression will tend to retain only one
#   feature out of many that are correlated."
math: yes
---




# Intro

First, let's read in the data from the last lesson.



~~~
library("here")
library("minfi")
if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
methylation <- readRDS(here("data/methylation.rds"))

methyl_mat <- t(assay(methylation))
age <- methylation$Age
~~~
{: .language-r}

# Why would we want to do feature selection?

In the previous lesson we did a kind of feature selection by doing
univariate analysis and thresholding by p-value/effect size.


However we might think there's some combination of methylation features
that combined explain age. For example, if we want to be able to predict age 
from methylation, that's a lot easier if we figure out what the contribution
of each feature is conditional on all others, rather than independent of
all others.

$$
    y_j = \beta_0 + \beta_1 X_1 + \dots \beta_p X_p + \epsilon_j
$$

However when the number of predictors is greater than the number of samples
(basically always true in genetics) it isn't possible to include everything!

What happens if we try to fit a model here?


~~~
fit <- lm(age ~ methyl_mat)
summary(fit)
~~~
{: .language-r}



~~~

Call:
lm(formula = age ~ methyl_mat)

Residuals:
ALL 37 residuals are 0: no residual degrees of freedom!

Coefficients: (4964 not defined because of singularities)
                          Estimate Std. Error t value Pr(>|t|)
(Intercept)               2640.474        NaN     NaN      NaN
methyl_matcg00075967      -108.216        NaN     NaN      NaN
methyl_matcg00374717      -139.637        NaN     NaN      NaN
methyl_matcg00864867        33.102        NaN     NaN      NaN
methyl_matcg00945507        72.250        NaN     NaN      NaN
 [ reached getOption("max.print") -- omitted 4996 rows ]

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:    NaN 
F-statistic:   NaN on 36 and 0 DF,  p-value: NA
~~~
{: .output}

So we can't do that mathematically, we have to find another way.

# Screening

What people sometimes do is to select variables based on correlation with
the outcome, or using a univariate modelling approach like we used in the 
previous lesson.
There's some problems with this! First, the p-values we get out of the final
model are meaningless because we're basically doing a 2-stage model and only
reporting one set of p-values (ignoring all the non-significant ones).
Secondly, we're likely to select a bunch of features that all have the same 
information!


~~~
nvar <- 1000
nobs <- 100
y_synth <- rnorm(nobs)
synth_mat <- matrix(rnorm(nobs * nvar), nrow = nobs, ncol = nvar)
cors <- apply(synth_mat, 2, function(col) cor(col, y_synth))
X_pred <- synth_mat[, abs(cors) > quantile(abs(cors), 0.99)]
summary(lm(y_synth ~ ., data = as.data.frame(X_pred)))
~~~
{: .language-r}



~~~

Call:
lm(formula = y_synth ~ ., data = as.data.frame(X_pred))

Residuals:
     Min       1Q   Median       3Q      Max 
-1.71211 -0.34571 -0.02708  0.41282  1.81269 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.01075    0.06595   0.163 0.870905    
V1          -0.06962    0.07014  -0.993 0.323544    
V2           0.10934    0.07021   1.557 0.122941    
V3           0.14148    0.06441   2.197 0.030649 *  
V4          -0.17722    0.06905  -2.567 0.011937 *  
V5           0.22423    0.06434   3.485 0.000765 ***
V6           0.14239    0.07701   1.849 0.067762 .  
V7          -0.18310    0.06849  -2.674 0.008928 ** 
V8           0.24941    0.07507   3.322 0.001297 ** 
V9           0.24049    0.06871   3.500 0.000730 ***
V10          0.16502    0.06174   2.673 0.008951 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.6482 on 89 degrees of freedom
Multiple R-squared:  0.5615,	Adjusted R-squared:  0.5122 
F-statistic: 11.39 on 10 and 89 DF,  p-value: 2.538e-12
~~~
{: .output}

Amazing, the results are highly significant! However this isn't rigorous and
can lead to problems so don't do it!


# Best subset selection

However, we could imagine trying each combination of features to find which
is the best combination. This works, but is really computationally demanding,
because blah blah number of permutations probably $p!$ but need to check.



~~~
library("leaps")
small_methyl <- methyl_mat[, 1:10]
fit_all <- regsubsets(x = small_methyl, y = age, really.big = TRUE)
summ <- summary(fit_all)
coef(fit_all, which.min(summ$rss))
~~~
{: .language-r}



~~~
(Intercept)  cg00374717  cg00864867  cg01027739  cg01353448  cg01584473 
 145.118554    1.073187   13.767298   22.683667    9.489186  -14.784050 
 cg01644850  cg01656216  cg01873645 
   4.853302    7.915842   -9.715682 
~~~
{: .output}

Let's try running BS on the full dataset.


~~~
fit_all <- regsubsets(x = methyl_mat, y = age, really.big = TRUE)
summ <- summary(fit_all)
coef(fit_all, which.min(summ$rss))
~~~
{: .language-r}

This doesn't really work in a reasonable time because of the number of possible
combinations!

<img src="../fig/bs_fs.png" width="500px" style="display: block; margin: auto;" />

Figure taken from [Hastie et al. (2020)](https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf),
published [here](https://doi.org/10.1214/19-STS733).


# Model metrics

In the example above we used RSS (residual sum of squares) to choose a model.
However when comparing models with different numbers of features, this is
problematic. We could, for example, keep adding features that marginally 
reduce the RSS (because adding a feature will never make it worse!) and
under this framework we'll always select the biggest model.

For example, if we have as many features as observations, the fit is always
perfect.


~~~
nvar <- 100
nobs <- 100
y_synth <- rnorm(nobs)
X_synth <- matrix(rnorm(nobs * nvar), nrow = nobs, ncol = nvar)
fit <- lm(y_synth ~ 0 + ., data = as.data.frame(X_synth))
sum(residuals(fit)^2)
~~~
{: .language-r}



~~~
[1] 0
~~~
{: .output}

There are other ways to measure model performance while accounting for the
complexity of the model. For example, adjusted $R^2$ is similar to the normal
R^2 measure that estimates the variation explained by the model, while also
accounting for the number of features. This is explained
in more detail in [the multiple regression lesson](https://carpentries-incubator.github.io/multiple-linear-regression-public-health/)

BIC and AIC also exist. There's also an issue of how well these things
will generalise beyond the present dataset.

> ## Exercise
>
> Select the best model based on BIC. How does this differ to the best RSS 
> model?
> 
> > ## Solution
> > 
> > 
> > 
> > ~~~
> > coef(fit_all, id = which.min(summ$bic))
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > (Intercept)  cg01353448 
> >    31.52262    10.92400 
> > ~~~
> > {: .output}
> > 
> > 
> > 
> > ~~~
> > coef(fit_all, id = which.min(summ$rss))
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > (Intercept)  cg00374717  cg00864867  cg01027739  cg01353448  cg01584473 
> >  145.118554    1.073187   13.767298   22.683667    9.489186  -14.784050 
> >  cg01644850  cg01656216  cg01873645 
> >    4.853302    7.915842   -9.715682 
> > ~~~
> > {: .output}
> {: .solution}
{: .challenge}

> # Selecting a model
> Yes, selecting a model with BIC on the training set is not ideal, we talk
> about cross-validation later.
{: .callout}

# Forward stepwise selection

Since BS is computationally hard, we can instead do an approximation.
One of these is forward stepwise selection.

Basically:

1. pick the most significant feature
2. fit a model with that feature and every other
3. if any are a significant improvement, pick the model that has the best improvement
   and return to 2.
4. otherwise stop

Here we apply it to a synthetic methylation dataset where we know the true
predictors.


~~~
## challenge 4: forward selection
## compare with true betas
if (!file.exists(here("data/synthetic.rds"))) {
  source(here("data/synthetic.R"))
}
synthetic <- readRDS(here("data/synthetic.rds"))

synth_mat <- t(assay(synthetic))
synth_age <- synthetic$age

fit_forward <- regsubsets(x = synth_mat, y = synth_age, method = "forward")
summ_forward <- summary(fit_forward)
est_coef_fwd <- coef(fit_forward, id = which.min(summ_forward$bic))
true_coefs <- rowData(synthetic)[names(est_coef_fwd)[-1], "true_beta"]
true_coefs
~~~
{: .language-r}



~~~
[1] -2.262349 -1.178890 -1.614445 -1.650195  1.056577  3.205232 -2.862240
[8]  2.100281
~~~
{: .output}



~~~
est_coef_fwd
~~~
{: .language-r}



~~~
(Intercept)   feature_6  feature_12  feature_18  feature_45  feature_65 
  40.737563   -1.672063   -1.644779   -1.724151   -1.541074    1.773289 
 feature_88  feature_89  feature_93 
   3.455787   -2.531996    2.454369 
~~~
{: .output}



~~~
all_coefs <- c(true_coefs, est_coef_fwd[-1])
plot(true_coefs, est_coef_fwd[-1], xlim = range(all_coefs), ylim = range(all_coefs))
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-03-forwardsyn-1.png" width="432" style="display: block; margin: auto;" />






> ## Exercise
> Perform forward subset selection on the methylation data.
> 
> Check the BICs. Select the best model based on BIC.
>
> > ## Solution
> > 
> > 
> > ~~~
> > ## challenge 4: forward selection
> > fit_forward <- regsubsets(x = methyl_mat, y = age, method = "forward")
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > Warning in leaps.setup(x, y, wt = weights, nbest = nbest, nvmax = nvmax, : 4964
> > linear dependencies found
> > ~~~
> > {: .warning}
> > 
> > 
> > 
> > ~~~
> > summ_forward <- summary(fit_forward)
> > est_coef_fwd <- coef(fit_forward, id = which.min(summ_forward$bic))
> > est_coef_fwd
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > (Intercept)  cg06493994  cg09516423  cg06839820  cg06102342  cg16279544 
> >   -9.712995   23.441089  -31.301058  -20.811742   18.060034   15.088039 
> >  cg17646874  cg15007454  cg03727836 
> >   11.454640    7.832117   -2.837760 
> > ~~~
> > {: .output}
> > 
> {: .solution}
{: .challenge}


# Reverse stepwise selection

If we have a model that we think is real and we want to slim it down, we
can do reverse subset selection.

`methylclock::coefHorvath` is the Horvath methylation age predictor
[Horvath (2013)](https://pubmed.ncbi.nlm.nih.gov/24138928/).


> ## Exercise
> 
> Do reverse subset selection and compare with the forward and reverse model.
> 
> > ## Solution
> >
> > 
> > ~~~
> > features <- methylclock::coefHorvath$CpGmarker
> > features <- intersect(features, colnames(methyl_mat))
> > methyl_horvath <- methyl_mat[, features[1:30]]
> > 
> > ## note about backward/both, not a challenge
> > fit_back <- regsubsets(x = methyl_horvath, y = age, method = "backward")
> > summ_back <- summary(fit_back)
> > 
> > summ_back$bic
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > [1] -11.827037 -11.841527 -10.671471 -12.328255 -12.596295 -11.690432 -10.603882
> > [8]  -9.830097
> > ~~~
> > {: .output}
> > 
> > 
> > 
> > ~~~
> > est_coef_back <- coef(fit_back, id = which.min(summ_back$bic))
> > true_coef <- setNames(
> >   methylclock::coefHorvath$CoefficientTraining,
> >   methylclock::coefHorvath$CpGmarker
> > )
> > plot(est_coef_back[-1], true_coef[names(est_coef_back)[-1]])
> > abline(0, 1)
> > abline(v = 0, lty = "dashed", col = "firebrick")
> > abline(h = 0, lty = "dashed", col = "firebrick")
> > ~~~
> > {: .language-r}
> > 
> > <img src="../fig/rmd-03-reverse-1.png" width="432" style="display: block; margin: auto;" />
> > 
> > ~~~
> > names(est_coef_back[-1])
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > [1] "cg02388150" "cg02489552" "cg03167275" "cg03760483" "cg04084157"
> > ~~~
> > {: .output}
> > 
> > 
> > 
> > ~~~
> > names(est_coef_fwd[-1])
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > [1] "cg06493994" "cg09516423" "cg06839820" "cg06102342" "cg16279544"
> > [6] "cg17646874" "cg15007454" "cg03727836"
> > ~~~
> > {: .output}
> > 
> > 
> > 
> > ~~~
> > intersect(names(est_coef_back[-1]), names(est_coef_fwd[-1]))
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > character(0)
> > ~~~
> > {: .output}
> {: .solution}
{: .challenge}




{% include links.md %}
