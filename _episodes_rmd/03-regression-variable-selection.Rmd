---
title: "Feature selection for regression"
teaching: 0
exercises: 0
questions:
- "Why would we want to find a subset of features
  that are associated with an outcome?"
- "How should we *not* select features?"
- "How can we iteratively find a good subset of our features
  variables to use for regression?"
- "What are some risks and downsides of iterative feature
  selection?"
objectives:
- "Understand multiple regression in a biomedical context."
- "Understand how to fit a stepwise regression model."
keypoints:
- "Sets of features can be more predictive and provide
  a better explanation than a single feature alone."
- "Stepwise regression allows us to find a set of features that
  are associated with an outcome (eg, age)."
- "Stepwise regression will tend to retain only one
  feature out of many that are correlated."
- "Stepwise regression is not very efficient."
math: yes
---


```{r, include=FALSE}
library("here")
source(here("bin/chunk-options.R"))
knitr_fig_path("03-")
```


In the previous lesson we did a kind of feature selection by doing
univariate analysis and thresholding by p-value/effect size.




However we might think there's some combination of methylation features
that combined explain age.

$$
    y_j = \beta_0 + \beta_1 X_1 + \dots \beta_p X_p + \epsilon_j
$$

However when the number of predictors is greater than the number of samples
(basically always true in genetics) it isn't possible to include everything!


```{r}
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("minfi")
    library("here")
    library("broom")
})

if (!file.exists(here("data/synthetic.rds"))) {
    source(here("data/synthetic.R"))
}
synthetic <- readRDS(here("data/synthetic.rds"))

X <- as.matrix(t(assay(synthetic)))
y <- synthetic$age
beta <- rowData(synthetic)$true_beta
names(beta) <- rownames(synthetic)

# ## challenge 3: fit y on x univariate
# ## compare with true betas
cc <- sapply(seq_len(ncol(X)), function(i) {
    coef(lm(y ~ X[, i]))[[2]]
})
plot(cc, beta, pch = 19, cex = 0.5)
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
```


What happens if we try to fit a model here?

```{r}
fit <- lm(y ~ X)
summary(fit)
```


What people sometimes do is to select variables based on correlation with
the outcome, or using a univariate modelling approach like we used in the 
previous lesson.
There's some problems with this! First, the p-values we get out of the final
model are meaningless because we're basically doing a 2-stage model and only
reporting one set of p-values (ignoring all the non-significant ones).
Secondly, we're likely to select a bunch of features that all have the same 
information!

```{r}
ncol <- 1000
nrow <- 100
y <- rnorm(100)
X_synth <- matrix(rnorm(nrow * ncol), nrow, ncol)
cors <- apply(X_synth, 2, function(col) cor(col, y))
X_pred <- X_synth[, abs(cors) > quantile(abs(cors), 0.99)]
summary(lm(y ~ ., data = as.data.frame(X_pred)))
```

Amazing, the results are highly significant! However this isn't rigorous and
can lead to problems so don't do it!


## Best subset selection

However, we could imagine trying each combination of features to find which
is the best combination. This works, but is really computationally demanding,
because blah blah number of permutations probably $p!$ but need to check.



```{r}
library("leaps")
small_x <- X[, 1:10]
fit_all <- regsubsets(x = small_x, y = y, really.big = TRUE)
coef(fit_all, which.min(s$rss))
```


> ## Exercise
>
> Try running BS on the full dataset.
>  
> > ## Solution
> > 
> > It won't run because BS is BS.
> {: .solution}
{: .challenge}


```{r}
knitr::include_graphics(here("fig/bs_fs_lasso.png"))
```

Figure taken from [Hastie et al. (2020)](https://doi.org/10.1214/19-STS733).


## Model metrics

In the example above we used RSS (residual sum of squares) to choose a model.
However when comparing models with different numbers of features, this is
problematic. We could, for example, keep adding features that marginally 
reduce the RSS (because adding a feature will never make it worse!) and
under this framework we'll always select the biggest model.

There are other ways to measure model performance while accounting for the
complexity of the model. For example, adjusted $R^2$ is the normal
R^2 measure that estimates the variation explained by the model while 
accounting for the number of variables.

BIC and AIC also exist.



> ## Exercise
>
> Select the best model based on BIC.
>  
> > ## Solution
> > 
> > 
```{r}
coef(fit_all, which.min(s$bic))
```
> {: .solution}
{: .challenge}




## Forward stepwise selection

Instead what we can do is forward stepwise selection.

Basically:

1. pick the most significant feature
2. fit a model with that feature and every other
3. if any are a significant improvement, pick the model that has the best improvement
   and return to 2.
4. otherwise stop


```{r}
## challenge 4: forward selection
## compare with true betas
xy <- as.data.frame(cbind(X, y = y))
int <- lm(y ~ 1, data = xy)
all <- lm(y ~ . + 0, data = xy)
forward <- step(
    int,
    scope = list(upper = formula(all), lower = formula(int)),
    direction = "forward",
    trace = 0
)
forward$anova
plot(coef(forward), beta[names(coef(forward))])
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
```



```{r}
if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
methylation <- readRDS(here("data/methylation.rds"))

y <- methylation$Age
X <- t(assay(methylation))
```



> ## Exercise
> Perform forward subset selection on the methylation data.
> 
> Check the BICs.
> > ## Solution
> > 
> > As above.
> {: .solution}
{: .challenge}

```{r}
## challenge 4: forward selection
## compare with true betas
xy <- as.data.frame(cbind(X, y = y))
int <- lm(y ~ 1, data = xy)
all <- lm(y ~ . + 0, data = xy)
forward <- step(
    int,
    scope = list(upper = formula(all), lower = formula(int)),
    direction = "forward",
    trace = 0
)
```

## Reverse stepwise

If we have a model that we think is real and we want to slim it down, we
can do reverse subset selection.

## Exercise

Do reverse subset selection and compare with the full model somehow.
```{r}
features <- methylclock::coefHorvath$CpGmarker
features <- intersect(features, colnames(X))
x_horvath <- X[, features]

## note about backward/both, not a challenge
fit_back <- regsubsets(x = x_horvath, y = y, method = "backward")

coef(fit_back, which.min(s$bic))



# all <- lm(y ~ . + 0, data = xy)
# backward <- step(
#     all,
#     scope = formula(all),
#     direction = "backward",
#     trace = 0
# )
# backward$anova
# plot(coef(backward), beta[names(coef(backward))])
# abline(0, 1)
# abline(v = 0, lty = "dashed", col = "firebrick")
# abline(h = 0, lty = "dashed", col = "firebrick")
```


{% include links.md %}
