---
title: "Regression with many features"
teaching: 60
exercises: 30
questions:
- "How can we apply regression methods in a high-dimensional setting?"
- "How can we control for the fact that we do many tests?"
- "How can we benefit from the fact that we have many variables?"
objectives:
- "Perform and critically analyse high dimensional regression."
- "Perform multiple testing adjustment."
- "Understand methods for shrinkage of noise parameters in
  high-dimensional regression."
keypoints:
- "Running many tests with high-dimensional data requires us to pay attention to 
   ..."
- "Multiple testing correction can enable us to account for many null hypothesis
    significance tests while retaining power."
- "Sharing information between features can increase power and reduce false 
    positives."
math: yes
---


```{r, include=FALSE}
library("here")
source(here("bin/chunk-options.R"))
knitr_fig_path("02-")
```

# Problem statement

In high-throughput studies, it's common to have one or more 
phenotypes that we want to relate to molecular features.
In general, we want to identify differences in the molecular
features (eg, gene expression, DNA methylation levels)
that are related to a phenotype or grouping of our samples.
Identifying molecular features that vary along with
phenotypes or groupings can allow us to understand how
phenotypes arise or manifest.

For example, we might want to identify genes that are
expressed at a higher level in mutant mice relative
to wild-type mice to understand the effect
of a mutation on cellular phenotypes.
Alternatively, we might have
samples from a set of patients, and wish to identify
epigenetic features that are different in young patients
relative to old patients, to help us understand how aging
manifests.

Using linear regression, it's possible to identify differences
like these . However, high-dimensional data like the ones we're
working with require some special considerations.

Ideally, we want to identify cases like this, where there is a
clear difference, and we probably "don't need" statistics:
```{r, echo=FALSE}
library("ggplot2")
set.seed(42)
n <- 10
x <- c(rnorm(n, 0), rnorm(n, 3))
group <- (2 * x) + rnorm(n)
ggplot() +
    aes(x = group, y = x) +
    geom_point() +
    labs(x = "Age", y = "Gene expression")
```

or equivalently for a discrete covariate:

```{r, echo=FALSE}
library("ggplot2")
set.seed(42)
n <- 10
x <- c(rnorm(n, 0), rnorm(n, 3))
group <- c(rep("A", n), rep("B", n))
ggplot() +
    aes(x = group, y = x, colour = group) +
    # geom_violin() +
    # geom_boxplot(width = 0.25) +
    geom_jitter(height = 0, width = 0.2) +
    labs(y = "Gene expression")
```

However, often due to small differences and small sample sizes,
the problem is a bit more difficult:
```{r, echo=FALSE}
library("ggplot2")
set.seed(66)
n <- 5
x <- c(rnorm(n, 0), rnorm(n, 1))
group <- c(rep("A", n), rep("B", n))
ggplot() +
    aes(x = group, y = x, colour = group) +
    # geom_violin() +
    # geom_boxplot(width = 0.25) +
    geom_jitter(height = 0, width = 0.2) +
    labs(y = "Gene expression")
```

And, of course, we often have an awful lot of features and need
to prioritise a subset of them! We need a rigorous way to
prioritise genes for further analysis.

# Linear regression (recap)

Linear regression is a tool we can use to quantify the relationship
between two variables. With one predictor variable $x$,
it amounts to the following equation:

$$
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

where $\epsilon_i$ is the *noise*, or the variation in $y$ that isn't explained
by the relationship we're modelling. We assume this noise follows a normal
distribution[^1], that is:

$$
    \epsilon_i \sim N(0, \sigma^2)
$$

[^1]: It's not hugely problematic if the assumption of normal residuals is violated.

We can also write this using linear algebra (matrices and vectors) as follows: 

$$
    y = X\beta + \epsilon
$$

Another way of saying this is that y follows a normal distribution with

$$
    y \sim N(X\beta, \sigma^2)
$$

Or, visually, that (for example) this is the distribution 
of new points conditional on their $x$ values:

```{r, echo=FALSE}
set.seed(42)
x <- cbind(rep(1, 100), rnorm(100))
beta <- rnorm(2)
sx <- seq(min(x), max(x), length.out = 200)
sy <- seq(min(x), max(x), length.out = 200)
fx <- cbind(rep(1, 200), sx) %*% beta

dens <- matrix(NA, 200, 200)
for (i in seq_along(sx)) {
    for (j in seq_along(sy)) {
        dens[i, j] <- dnorm(sy[[j]], mean = fx[[i]])
    }
}
image(sx, sy, dens, col = viridis::viridis(100, option = "magma"), xlab="x", ylab="y")
```


> ## Exercise
>
>
> Launch `shinystats::regressionApp` and adjust the parameters.
> 
> 2. How does the degree of noise affect the level of certainty in the fitted
>   trend?
> 3. With a small number of observations, how strong does the relationship need
>   to be (or how small the noise) before it is significant?
> 4. With a large number of observations, how weak of an effect can you detect?
>   Is a really small effect (0.1 slope) really "significant" in the way you'd
>   use that word conversationally?
>
> > ## Solution
> > todo: plot examples for each question
> {: .solution}
{: .challenge}


```{r, echo=FALSE}
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("qvalue")
    library("minfi")
    library("here")
    library("FlowSorted.Blood.EPIC")
    library("IlluminaHumanMethylationEPICmanifest")
    library("IlluminaHumanMethylationEPICanno.ilm10b4.hg19")
    library("ExperimentHub")
    library("here")
    library("broom")
})
```

# Data


For the following few episodes, we'll be working with human
DNA methylation data from flow-sorted blood samples.
DNA methylation assays measure, for many sites in the genome,
the proportion of DNA that carries a methyl mark.

In this case, the methylation data come in the form of a matrix
of normalised methylation levels (M-values, for the technical among
you). Along with this, we have a number of sample phenotypes
(eg, age in years, BMI).

The following code will read in the data for this episode.

```{r}
suppressPackageStartupMessages({
    library("minfi")
    library("limma")
    library("here")
    library("broom")
})

if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
norm <- readRDS(here("data/methylation.rds"))

norm <- norm
X <- getM(norm)
```

The distribution of these M-values looks like this:

```{r}
hist(X, breaks = "FD", xlab = "M-value")
```

In high-throughput experiments like this, we commonly have one or more 
phenotypes that we want to relate to molecular features.
In this case, these phenotypes are as follows:

```{r}
knitr::kable(head(colData(norm)))
```

In this case, we will focus on age in years. The association between
age and methylation status in blood samples has been studied extensively,
and is actually a good case-study in how to perform some of the techniques
we will cover in this lesson.

The distribution of age in these samples is as follows:

```{r}
y <- log(norm$Age)
hist(y, breaks = "FD", xlab = "log(Age)")
```

```{r}
library("ComplexHeatmap")
order <- order(y)
y_ord <- y[order]
X_ord <- X[, order]
Heatmap(X_ord,
    cluster_columns = FALSE,
    # cluster_rows = FALSE,
    name = "M-value",
    col = RColorBrewer::brewer.pal(10, "RdYlBu"),
    top_annotation = columnAnnotation(
        log_Age = y_ord
    ),
    show_row_names = FALSE,
    show_column_names = FALSE,
    row_title = "Feature",
    column_title =  "Sample",
    use_raster = FALSE
)
```


> ## Measuring DNA Methylation
> 
> DNA methylation is an epigenetic modification of DNA.
> Generally, we are interested in the proportion of 
> methylation at many sites or regions in the genome.
> DNA methylation microarrays, as we are using here,
> measure DNA methylation using two-channel microarrays,
> where one channel captures signal from methylated
> DNA and the other captures unmethylated signal.
> These data can be summarised
> as "Beta values" ($\beta$ values), which is the ratio
> of the methylated signal to the total signal 
> (methylated plus unmethylated).
> The $\beta$ value for site $i$ is calculated as
> 
> $$
>     \beta_i = \frac{
>         m_i
>     } {
>         u_{i} + m_{i}
>     }
> $$
> 
> where $m_i$ is the methylated signal for site $i$ and
> $u_i$ is the unmethylated signal for site $i$.
> $\beta$ values take on a value in the range 
> $[0, 1]$, with 0 representing a completely unmethylated 
> site and 1 representing a completely methylated site.
> 
> The M-values we use here are the $\log_2$ ratio of 
> methylated versus unmethylated signal:
>
> $$
>     M_i = \log_2\left(\frac{m_i}{u_i}\right)
> $$
> 
> M-values are not bounded to an interval as Beta-values
> are, and therefore may be less problematic for 
> statistical treatment.
{: .callout}


# Running linear regression

We have a matrix of methylation values $X$ and a vector of ages in years $y$.
One way to model this is to see if we can "predict" methylation using age.
Formally we'd describe that as:

$$
    X_{i,j} = \beta_0 + \beta_1 y_j + \epsilon_i
$$
where $y_j$ is the age of sample $j$.

You may remember how to fit this model from a previous lesson, and how to
get more information from the model object:

```{r}
fit <- lm(X[1, ] ~ y)
summary(fit)
```


We can also use `broom` to extract information about
the coefficients in this model:

```{r}
library("broom")
tidy(fit)
```

We have a lot of features, though! This is what it looks like if we do that
for every feature.

```{r}
dfs <- lapply(seq_len(nrow(X)),
    function(i) {
        df <- tidy(lm(X[i, ] ~ y))[2, ]
        df$term <- rownames(X)[[i]]
        df
    }
)
df_all <- do.call(rbind, dfs)
plot(df_all$estimate, -log10(df_all$p.value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
```

# Multiple testing

## The problem of multiple tests

With such a large number of features, we often want some way
to decide which features are "interesting" or "significant"
for further study.

To demonstrate this, it's useful to consider what happens if
we scramble age and run the same test again:

```{r}
y_perm <- y[sample(ncol(X), ncol(X))]
dfs <- lapply(seq_len(nrow(X)),
    function(i) {
        df <- tidy(lm(X[i, ] ~ y_perm))[2, ]
        df$term <- rownames(X)[[i]]
        df
    }
)
df_all <- do.call(rbind, dfs)
plot(df_all$estimate, -log10(df_all$p.value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
```


> ## Exercise
>
> 
> 1. If we run 10,000 tests under the null hypothesis,
>    how many of them (on average) will be statistically
>    significant at a threshold of $p < 0.05$?
> 2. Why would we want to be conservative in labelling features
>    as significantly different?
>    By conservative, we mean to err towards labelling true
>    differences as "not significant" rather than vice versa.
> 3. How could we account for a varying number of tests to
>    ensure "significant" changes are truly different? 
> 
> > ## Solution
> > 1. By default we expect $`r nrow(X)` \times 0.05 = `r nrow(X)*0.05`$
> >    features to be statistically significant under the null hypothesis,
> >    because p-values should always be uniformly distributed under
> >    the null hypothesis.
> > 2. Features that we label as "significantly different" will often
> >    be reported in manuscripts. We may also spend time and money
> >    investigating them further, computationally or in the lab.
> >    Therefore, spurious results have a real cost for ourselves and
> >    for others.
> > 3. One approach to controlling for the number of tests is to
> >    divide our significance threshold by the number of tests
> >    performed. This is termed "Bonferroni correction" and
> >    we'll discuss this further now.
> {: .solution}
{: .challenge}


## Adjusting for multiple comparisons

When performing many statistical tests to
categorise features, we're effectively classifying
features.

|              |Predicted true|Predicted false|
|-------------:|-------------:|--------------:|
|Actually true |True positive |False negative |
|Actually false|False positive|True negative  |


The t-statistic for a linear model like this is given by:

$$
    t_{ij} = \frac{\hat{\beta}_{ij}}{SE(\hat{\beta}_{ij})}
$$

$SE(\hat{\beta}_{ij})$ measures the uncertainty we have in our effect
size estimate. 


Knowing what distribution these t-values follow under the null
hypothesis allows us to determine how unlikely it would be for
us to observe what we have under those circumstances.

# Sharing information

One idea is to take advantage of the fact that we're doing all these tests 
at once. We can leverage this fact to perform *shrinkage* of some model
parameters. Shrinkage methods can be complex to implement and understand,
but it's good to understand why these approaches may be more precise 
and sensitive than the naive approach of fitting a model to each feature
separately.



The insight of shrinkage methods is to realise that variance parameters
like these are probably similar between genes within the same experiment. This
enables us to share information between genes to get more robust
estimators.

In this case, 

not $j$ but rather $k$? for predictor.
$$
    t_{ij} = \frac{\hat{\beta}_{ij}}{s_i \sqrt{v_{ij}}}
$$

$s_i^2$ is the variance of residuals.


Similarly, DESeq2 shares information between genes
to *shrink* estimates of a noise parameter, in that case to model counts.

```{r}
## age - strong comparison
design <- model.matrix(~y)
colnames(design) <- c("intercept", "age")
fit <- lmFit(X, design = design)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 2, number = nrow(fit))
plot(tt1$logFC, -log10(tt1$P.Value),
    xlab = "Effect size", ylab = bquote(-log[10](p)),
    pch = 19
)
```

```{r echo = FALSE, eval=FALSE}
q <- qvalue(tt1$P.Value)
hist(q)
```

> ## Exercise
> Launch `shinystats::limmaApp` and adjust the parameters. 
> 
> Discuss the output in groups. Consider the following questions:
> 
> 1. How does the number of features affect the relationship between these two 
>    similar methods?
> 2. What about the number of samples?
> 3. When ranking genes, why would we want to downrank the most significant and
>    uprank some with more moderate changes?
> 
> > ## Solution
> > 
> > 1. With more features, the amount of shrinkage increases.
> > 2. With more samples, the shrinkage is weaker and the difference between the
> >    methods is smaller.
> > 3. Because the p-value relies on the effect size estimate *and* its standard
> >    error, a very small standard error by chance (with few replicates) can
> >    lead to a very small p-value. "Moderating" or shrinking the standard errors
> >    brings these more in line with features that have a similar effect size 
> >    but larger standard error.
> {: .solution}
{: .challenge}


> ## Shrinkage
> 
> Shrinkage is an intuitive term for an effect
> of a broad range of statistical models.
> Often, shrinkage is induced by a *multilevel*
> modelling approach or by *Bayesian* methods.
> 
> The general idea is that these models incorporate 
> information about the hierarchical structure of the
> data into account when fitting the parameters.
> The sharing of information is accomplished because we use
> this information to formulate the model.
> 
> For example in `DESeq2`, the authors used the observation
> that genes with similar expression counts in RNAseq data
> have similar *dispersion*, and a better estimate of
> these dispersion parameters makes inference about
> fold changes much more stable.
> Similarly, in `limma` the authors made the assumption that
> in the absence of biological effects, we can often expect the
> technical variation between genes to be similar, and better
> estimates of variability allow us to prioritise genes
> in a more reliable way.
> 
> There are many good resources to learn about this type of approach,
> including:
> 
> - [a blog post by TJ Mahr](https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/)
> - [a book by David Robinson](https://gumroad.com/l/empirical-bayes)
> - [a (relatively technical) book by Gelman and Hill](http://www.stat.columbia.edu/~gelman/arm/)
{: .callout}

{% include links.md %}
