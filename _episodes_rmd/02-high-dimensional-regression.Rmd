---
title: "High dimensional regression"
teaching: 0
exercises: 0
questions:
- "How can we apply regression methods in a high-dimensional setting?"
- "How can we control for the fact that we do many tests?"
- "How can we benefit from the fact that we have many variables?"
- "How can we find a good subset of variables to use for regression?"
objectives:
- "Perform and critically analyse high dimensional regression."
- "Perform multiple testing adjustment."
- "Perform and critically analyse penalised regression."
keypoints:
- "Multiple testing correction can enable us to account for many null hypothesis
    significance tests while retaining power."
- "Sharing information between features can increase power and reduce false 
    positives."
- "Modelling features together can help to identify a subset of features
    that contribute to the outcome."
math: yes
---


```{r, include=FALSE}
library("here")
source(here("bin/chunk-options.R"))
knitr_fig_path("02-")
```


Linear regression with one predictor variable $x$ comprises the following 
equation

$$
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$


where $\epsilon_i$ is the *noise*, or the variation in $y$ that isn't explained
by the relationship we're modelling. We assume this noise follows a normal
distribution, that is:

$$
    \epsilon_i \sim N(0, \sigma^2)
$$

We can also write this using linear algebra (matrices and vectors) as follows: 

$$
    y = X\beta + \epsilon
$$

Another way of saying this is that y follows a normal distribution with

$$
    y \sim N(X\beta, \sigma^2)
$$



> ## Exercise
> Launch `shinystats::regressionApp` and adjust the parameters.
> 
> How does the degree of noise affect the level of certainty in the fitted
> trend? What about the number of observations?
> 
> > ## Solution
> > ??? What do I put here...
> {: .solution}
{: .challenge}


Do linear regression on each feature.

```{r}
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("qvalue")
    library("minfi")
    library("here")
    library("FlowSorted.Blood.EPIC")
    library("IlluminaHumanMethylationEPICmanifest")
    library("IlluminaHumanMethylationEPICanno.ilm10b4.hg19")
    library("ExperimentHub")
    library("here")
    library("broom")
})

if (!file.exists(here("data/FlowSorted_Blood_EPIC.rds"))) {
    source(here("data/methylation.R"))
}
norm <- readRDS(here("data/FlowSorted_Blood_EPIC.rds"))

lim <- norm
y <- lim$Age
X <- getM(lim)

dfs <- lapply(1:nrow(lim),
    function(i) {
        # cat(i, "/", nrow(X), "\n")
        df <- tidy(lm(X[i, ] ~ y))[2, ]
        df$term <- rownames(X)[[i]]
        df
    }
)
df_all <- do.call(rbind, dfs)
plot(df_all$estimate, -log10(df_all$p.value))
```


```
## age - strong comparison
design <- model.matrix(~lim$Age)
colnames(design) <- c("intercept", "age")
fit <- lmFit(getM(lim), design = design)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 2, number = nrow(fit))

plot(tt1$logFC, -log10(tt1$P.Value))
q <- qvalue(tt1$P.Value)
hist(q)
```


> ## Exercise
> Perform forward subset selection on the methylation data.
> 
> 
> > ## Solution
> > ```{r}
> > library("Seurat")
> > ```
> > ??? What do I put here...
> {: .solution}
{: .challenge}



```{r}
## challenge 2:
## Create a set of normal predictors, X
## then create a normally distributed outcome, y
## that depends on a subset of X
noise_sd <- 2
npred <- 99
frac <- 0.2
X <- replicate(npred - 1, rnorm(nobs, mean = 0, sd = 1))
colnames(X) <- paste0("predictor_", 1:(npred-1))
noise <- rnorm(nobs, mean = 0, sd = noise_sd)
X <- cbind(intercept = rep(1, nobs), X)
beta <- rep(0, npred)
names(beta) <- colnames(X)
ind <- as.logical(rbinom(npred, 1, frac))
beta[ind] <- rnorm(sum(ind)) + sample(c(-2, 2), sum(ind), replace=TRUE)
y <- ((X %*% beta) + noise)[, 1]
```

```{r}
## challenge 3: fit y on x univariate
## compare with true betas
cc <- sapply(1:ncol(X), function(i) {
    coef(lm(y ~ X[, i]))[[2]]
})
plot(cc, beta, pch = 19, cex = 0.5)
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")
```


```{r}
## challenge 4: forward selection
## compare with true betas
xy <- as.data.frame(cbind(X, y = y))
int <- lm(y ~ 1, data=xy)
all <- lm(y ~ . + 0, data=xy)
forward <- step(
    int,
    scope = list(upper = formula(all), lower = formula(int)),
    direction = "forward",
    trace = 0
)
forward$anova
plot(coef(forward), beta[names(coef(forward))])
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")
```

```{r}
## note about backward/both, not a challenge
all <- lm(y ~ . + 0, data=xy)
backward <- step(
    all,
    scope = formula(all),
    direction = "backward",
    trace = 0
)
backward$anova
plot(coef(backward), beta[names(coef(backward))])
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")
```

```{r}
## Challenge 5:
## one of these...? probably lasso
library("glmnet")
ridge <- cv.glmnet(X[, -1], y, alpha = 0)
lasso <- cv.glmnet(X[, -1], y, alpha = 1)
elastic <- cv.glmnet(X[, -1], y, alpha = 0.5, intercept = FALSE)
plot(coef(lasso, s = lasso$lambda.1se)[, 1], beta)
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")

plot(coef(elastic, s = elastic$lambda.1se)[, 1], beta)
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")

plot(coef(ridge, s = ridge$lambda.1se)[, 1], beta)
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")
                                      
```









```{r}
x <- t(getM(norm))
y <- as.numeric(factor(norm$smoker)) - 1

fit <- cv.glmnet(x = x, y = y, family="binomial")

c <- coef(fit, s = fit$lambda.1se)
c[c[, 1] != 0, 1]


y <- norm$Age
fit <- cv.glmnet(x = x, y = y)

c <- coef(fit, s = fit$lambda.1se)
coef <- c[c[, 1] != 0, 1]

plot(y, x[, names(which.max(coef[-1]))])
```



```{r, eval=FALSE, echo=FALSE}
design <- model.matrix(~0 + lim$bmi_clas)
colnames(design) <- gsub("lim$bmi_clas", "", colnames(design), fixed=TRUE)

fit <- lmFit(getM(lim), design = design)
contrasts <- makeContrasts(
    Overweight - Normal,
    Obese - Normal,
    levels = design
)
fit <- contrasts.fit(fit, contrasts)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 1, number = nrow(fit))
tt2 <- topTable(fit, coef = 2, number = nrow(fit))

q <- qvalue(tt2$P.Value)
tt2$qvalue <- q$qvalue


design <- model.matrix(~0 + lim$smoker)
colnames(design) <- gsub("lim$smoker", "", colnames(design), fixed=TRUE)



fit <- lmFit(getM(lim), design = design)
contrasts <- makeContrasts(
    Yes - No,
    levels = design
)
fit <- contrasts.fit(fit, contrasts)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 1, number = nrow(fit))


q <- qvalue(tt1$P.Value)
hist(q)

plot(tt1$logFC, -log10(tt1$P.Value))
```



{% include links.md %}

