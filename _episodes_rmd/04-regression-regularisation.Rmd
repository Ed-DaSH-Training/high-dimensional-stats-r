---
title: "Regularised regression with many features"
teaching: 0
exercises: 0
questions:
- "Can we fit a model that accounts for and selects many features?"
- "How does regularisation work?"
- "What are some considerations for a regularised model?"
objectives:
- "Understand the benefits of regularised models."
- "Understand how different types of regularisation work."
- "Perform and critically analyse penalised regression."
keypoints:
- "Regularisation is a way to avoid the problems of stepwise
  or iterative model building processes."
- "Modelling features together can help to identify a subset of features
    that contribute to the outcome."
math: yes
---




```{r, include=FALSE}
library("here")
source(here("bin/chunk-options.R"))
knitr_fig_path("04-")
```

```{r}
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("minfi")
    library("here")
    library("broom")
})

if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
norm <- readRDS(here("data/methylation.rds"))

lim <- norm
y <- lim$Age
X <- getM(lim)
```

In the previous 

Another way of modelling these data is to model age as 

$$
    y_j = \beta_0 + \beta_1 X_1 + \dots \beta_p X_p + \epsilon_j
$$

However when the number of predictors is greater than the number of samples
(basically always true in genetics) it isn't possible to include everything!

There are some techniques that you can use to find a set of predictors!

- screening (correlation etc): bad, don't do
- screening (variance): not necessarily bad if the screening variable is sensible
- forward/reverse/best subset selection

```{r}
if (!file.exists(here("data/synthetic.rds"))) {
    source("data/synthetic.R")
}
synthetic <- readRDS("data/synthetic.rds")
```


```{r}
## Challenge 5:
## one of these...? probably lasso
library("glmnet")
ridge <- cv.glmnet(X[, -1], y, alpha = 0)
lasso <- cv.glmnet(X[, -1], y, alpha = 1)
elastic <- cv.glmnet(X[, -1], y, alpha = 0.5, intercept = FALSE)
plot(coef(lasso, s = lasso$lambda.1se)[, 1], beta)
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")

plot(coef(elastic, s = elastic$lambda.1se)[, 1], beta)
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")

plot(coef(ridge, s = ridge$lambda.1se)[, 1], beta)
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
```









```{r}
x <- t(getM(norm))
y <- as.numeric(factor(norm$smoker)) - 1

fit <- cv.glmnet(x = x, y = y, family = "binomial")

c <- coef(fit, s = fit$lambda.1se)
c[c[, 1] != 0, 1]


y <- norm$Age
fit <- cv.glmnet(x = x, y = y)

c <- coef(fit, s = fit$lambda.1se)
coef <- c[c[, 1] != 0, 1]

plot(y, x[, names(which.max(coef[-1]))])
```



```{r, eval=FALSE, echo=FALSE}
design <- model.matrix(~0 + lim$bmi_clas)
colnames(design) <- gsub("lim$bmi_clas", "", colnames(design), fixed = TRUE)

fit <- lmFit(getM(lim), design = design)
contrasts <- makeContrasts(
    Overweight - Normal,
    Obese - Normal,
    levels = design
)
fit <- contrasts.fit(fit, contrasts)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 1, number = nrow(fit))
tt2 <- topTable(fit, coef = 2, number = nrow(fit))

q <- qvalue(tt2$P.Value)
tt2$qvalue <- q$qvalue


design <- model.matrix(~0 + lim$smoker)
colnames(design) <- gsub("lim$smoker", "", colnames(design), fixed = TRUE)



fit <- lmFit(getM(lim), design = design)
contrasts <- makeContrasts(
    Yes - No,
    levels = design
)
fit <- contrasts.fit(fit, contrasts)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 1, number = nrow(fit))


q <- qvalue(tt1$P.Value)
hist(q)

plot(tt1$logFC, -log10(tt1$P.Value))
```


> ## Selecting hyperparameters
> 
> There are various methods to select the "best"
> value for $\lambda$. One idea is to split
> the data into $K$ chunks. We then use $K-1$ of
> these as the training set, and $the remaining $1$ chunk
> as the test set. Repeating this process for each of the
> $K$ chunks produces more variability.
> 
> ```{r}
> knitr::include_graphics(here("fig/cross_validaiton.svg"))
> ```
>
> To be really rigorous, we could even repeat this *cross-validation*
> process a number of times! This is termed "repeated cross-validation".
{: .callout}


{% include links.md %}
